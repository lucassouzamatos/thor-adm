---

# This module configures basic management stuff.
# It needs an user with custom IAM policy, some of them are:
# - CreateKeyPair : allows the creation of the SSH keys locally and assining it 
# to amazon key-pair, thus we can specify these keys to connect to newly instances.
# - CreateSecurityGroups : allow use open communication to instances from our IP block.
#
# Because of that, you may create a special user to execute such operations. We already provide
# an IAM policy that handles all of this.

- hosts: localhost
  tasks:
    # gather ubuntu image id 
    - block:
        - ec2_ami_info:
            filters:
              owner-id: 099720109477  # public available images
              name: 'ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*'
          register: ami_ubuntu

        - set_fact: 
            image_id: "{{ ami_ubuntu.images[0].image_id }}"
      when: image_id is not defined
    
    # gather security group ids
    # there are two paths to obtain the groups ids, such as:
    #   - user is regular, with ssh access from home.
    #   - user is a vpn operator, so one of his/her instance is the vpn server,
    #   which needs access from home and teammate's home. SSH access is limited
    #   to operator home only.
    - block:
        - include_role:
            name: my_pub_cidr

        - block:  
            - ec2_group:
                name: vpn-connection-hole
                description: Default SSH and Wireguard rules for home connection
                rules:
                  - proto: tcp
                    ports: 22
                    cidr_ip: "{{ my_cidr }}"

                  - proto: udp
                    ports: "{{ wg_listen_port }}"
                    cidr_ip: "{{ [my_cidr] + teammate_cidrs }}"
              register: sg_vpn

            - ec2_group:
                name: intranet-ssh-hole
                description: Default SSH rules for private network
                rules:
                  - proto: tcp
                    ports: 
                      - 22
                      - 3080
                    cidr_ip: 0.0.0.0/0  # XXXX is it in some way insecure?
              register: sg_ssh
            
            - set_fact:
                sg_ids:
                  - "{{ sg_vpn.group_id }}"
                  - "{{ sg_ssh.group_id }}"
          when: is_vpn_operator

        - block:
            - ec2_group:
                name: ssh-hole
                description: Default SSH rules for home connection
                rules:
                  - proto: tcp
                    ports: 22
                    cidr_ip: "{{ my_cidr }}"
              register: sg_ssh
            
            - set_fact:
                sg_ids:
                  - "{{ sg_ssh.group_id }}"
          when: not is_vpn_operator
      when: sg_ids is not defined

    # gather vpc subnet id, there is no need to create a custom subnet network by now, so
    # just get a random one
    - block:
        # SaoPaulo region sa-east-1b does not support t2.micro instances
        - set_fact:
            subnet_id: "{{ ['sa-east-1a', 'sa-east-1c'] | shuffle | first }}"
          when: is_sp_region
        
        - block:
          - ec2_vpc_subnet_info:
            register: vpc_subnets

          - set_fact:
              subnet_id: "{{ (vpc_subnets.subnets | shuffle | first).subnet_id }}"
          when: not is_sp_region

      when: subnet_id is not defined
      vars: 
        is_sp_region: "{{ inst_region == 'sa-east-1' }}" 
    
    # generate a new ssh key when needed
    # used one key for all of the instances, including vpn server
    # assuming once it is compromissed, it could impersonate gns3-server
    # to create a VM inside the host machine and exploit qemu or even gn3-server
    # implementation and get access to the host machine.
    #
    # TODO: above could be avoided with password authenticated gns3-server
    - block:
        - set_fact:
            ssh_private_key: "{{ '/'.join([keys_dir, 'id_rsa']) }}"
        
        - stat:
            path: "{{ ssh_private_key + '.pub' }}" 
          register: gen_ssh_key_stat

        # only create a new ssh key when one is missing
        - openssh_keypair:
            path: "{{ ssh_private_key }}"
          when: not gen_ssh_key_stat.stat.exists

        - set_fact:
            ssh_pubkey: "{{ lookup('file', ssh_private_key + '.pub') }}"
      when: key_id is not defined and ssh_pubkey is not defined
      vars:
        - keys_dir: "{{ default_ssh_dir | expanduser }}"

    # gather key pair id, which is the key name
    - block:
      - ec2_key:
          name: home-conn-ssh-pubkey-{{ 99999 | random(start=1, step=5) }}
          key_material: "{{ ssh_pubkey }}"
          force: false
        register: ec2_key_pair
  
      - set_fact:
          key_id: "{{ ec2_key_pair.key.name }}"
      when: key_id is not defined

    - name: create deployer user with defined instance permissions
      iam_managed_policy:
        policy_name: EC2UbuntuFreeTierDeployer
        policy_description: Allows user to deploy free-tier ubuntu instances
        policy: "{{ lookup('template', 'policies/ubuntu-free-tier.json.j2') }}"
      register: ec2_deployer_policy
    
    - name: create deployer user assigned to created group
      iam_user:
        name: ubuntu-sim-deployer
        state: present
        managed_policy:
          - "{{ ec2_deployer_policy.policy.arn }}"

    - name: ensure variables directory exists
      file:
        path: vars/
        state: directory

    - name: save created objects to disk, for further use
      copy:
        dest: "vars/ubuntu-sim-deployer.yml"
        mode: 0644
        backup: yes
        content: |
          ---
          image_id: {{ image_id }}
          key_id: {{ key_id }}
          sg_ids: {{ sg_ids }}
          subnet_id: {{ subnet_id }}
          volume_size: {{ volume_size }}
      register: deployer_var

    - name: include deployer variables
      include_vars:
        file: "{{ deployer_var.dest }}"