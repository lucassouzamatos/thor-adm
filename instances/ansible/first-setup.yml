---

# This module configures basic management stuff.
# It needs an user with custom IAM policy, some of them are:
# - CreateKeyPair : allows the creation of the SSH keys locally and assining it 
# to amazon key-pair, thus we can specify these keys to connect to newly instances.
# - CreateSecurityGroups : allow use open communication to instances from our IP block.
#
# Because of that, you may create a special user to execute such operations. We already provide
# an IAM policy that handles all of this.

- hosts: localhost
  tasks:
    - include_role:
        name: aws_config
      vars:
        config: "{{ setup_manager_config }}"

    - name: check where the generated setup variables file is present
      stat:
        path: "{{ default_setup_variables_file }}"
      register: default_setup_vars_stat

    - name: include deployer setup variables
      include_vars:
        file: "{{ default_setup_variables_file }}"
      when: default_setup_vars_stat.stat.exists

    # gather ubuntu image id 
    - block:
        - ec2_ami_info:
            filters:
              owner-id: 099720109477  # public available images
              name: 'ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*'
          register: ami_ubuntu

        - set_fact: 
            image_id: "{{ ami_ubuntu.images[0].image_id }}"
      when: image_id is not defined
    
    # gather security group ids
    # there are two paths to obtain the groups ids, such as:
    #   - user is regular, with ssh access from home.
    #   - user is a vpn operator, so one of his/her instance is the vpn server,
    #   which needs access from home and teammate's home. SSH access is limited
    #   to operator home only.
    - block:
        - name: ensure cache directory exists
          file:
            path: "{{ cache_dir }}"
            state: directory

        - include_role:
            name: my_pub_cidr

        - block:  
            - ec2_group:
                name: vpn-connection-hole
                description: Default SSH and Wireguard rules for home connection
                rules:
                  - proto: tcp
                    ports: 22
                    cidr_ip: "{{ my_cidr }}"

                  - proto: udp
                    ports: "{{ wg_listen_port }}"
                    cidr_ip: "{{ [my_cidr] + teammate_cidrs }}"
              register: sg_vpn

            - ec2_group:
                name: intranet-ssh-hole
                description: Default SSH rules for private network
                rules:
                  - proto: tcp
                    ports: 
                      - 22
                      - 3080
                    cidr_ip: 0.0.0.0/0  # XXXX is it in some way insecure?
              register: sg_ssh
            
            - set_fact:
                sg_ids:
                  - "{{ sg_vpn.group_id }}" # vpn server group 
                  - "{{ sg_ssh.group_id }}" # dummy instances group
          when: is_vpn_operator

        - block:
            - ec2_group:
                name: ssh-hole
                description: Default SSH rules for home connection
                rules:
                  - proto: tcp
                    ports: 22
                    cidr_ip: "{{ my_cidr }}"
              register: sg_ssh
            
            - set_fact:
                sg_ids:
                  - "{{ sg_ssh.group_id }}"
          when: not is_vpn_operator
      when: sg_ids is not defined 
    
    # gather vpc subnet id, there is no need to create a custom subnet network by now, so
    # just get a random one
    - block:
        # SaoPaulo availability zone 'sa-east-1b' does not support t2.micro instances
        - block:
          - ec2_vpc_subnet_info:
              filters:
                availability-zone: "{{ ['sa-east-1a', 'sa-east-1c'] | shuffle | first }}" 
            register: ec2_sp_subnet
           
          - set_fact:
              the_subnet: "{{ ec2_sp_subnet.subnets | first }}"
          when: is_sp_region
        
        - block:
          - ec2_vpc_subnet_info:
            register: vpc_subnets

          - set_fact:
              the_subnet: "{{ vpc_subnets.subnets | shuffle | first }}"
          when: not is_sp_region

        - set_fact:
            subnet_id: "{{ the_subnet.id }}"
            subnet_cidr_block: "{{ the_subnet.cidr_block }}"

      when: subnet_id is not defined
      vars: 
        is_sp_region: "{{ inst_region == 'sa-east-1' }}"

    # automatically gather subnet cidr block when user defined subnet_id
    - block:
        - ec2_vpc_subnet_info:
            subnet_ids: "{{ subnet_id }}"
          register: the_user_subnet

        - set_fact:
            subnet_cidr_block: "{{ (the_user_subnet.subnets | first).cidr_block }}"
      when: subnet_cidr_block is not defined

    # generate a new ssh key when needed
    # used one key for all of the instances, including vpn server
    # assuming once it is compromissed, it could impersonate gns3-server
    # to create a VM inside the host machine and exploit qemu or even gn3-server
    # implementation and get access to the host machine.
    #
    # TODO: above could be avoided with password authenticated gns3-server
    - block:
        - set_fact:
            ssh_private_key: "{{ [keys_dir, 'id_rsa'] | join('/') }}"
        
        - stat:
            path: "{{ ssh_private_key + '.pub' }}" 
          register: gen_ssh_key_stat

        - name: ensure ssh directory exists
          file:
            path: "{{ keys_dir }}"
            state: directory

        # only create a new ssh key when one is missing
        - openssh_keypair:
            path: "{{ ssh_private_key }}"
          when: not gen_ssh_key_stat.stat.exists

        - set_fact:
            ssh_pubkey: "{{ lookup('file', ssh_private_key + '.pub') }}"
      when: key_id is not defined and ssh_pubkey is not defined
      vars:
        - keys_dir: "{{ default_ssh_dir | expanduser }}"

    # gather key pair id, which is the key name
    - block:
      - ec2_key:
          name: home-conn-ssh-pubkey-{{ 99999 | random(start=1, step=5) }}
          key_material: "{{ ssh_pubkey }}"
          force: false
        register: ec2_key_pair
  
      - set_fact:
          key_id: "{{ ec2_key_pair.key.name }}"
      when: key_id is not defined

    - name: create deployer policy with defined instance permissions
      iam_managed_policy:
        policy_name: EC2UbuntuFreeTierDeployer
        policy_description: Allows user to deploy free-tier ubuntu instances
        policy: "{{ lookup('template', 'policies/ubuntu-free-tier.json.j2') }}"
      register: ec2_deployer_policy
    
    - name: create deployer user assigned to created group
      iam_user:
        name: "{{ users.simulation_deployer }}"
        state: present
        managed_policy:
          - "{{ ec2_deployer_policy.policy.arn }}"
      register: sim_deployer_user

    - set_fact:
        should_create_deployer_keys: "{{ sim_deployer_user.changed }}"
      
    - set_fact:
        should_create_deployer_keys: true
      tags:
        - never
        - create-deployer-keys

    - block:
      - block:
        - name: list deployer access keys
          shell: aws iam list-access-keys --user-name {{ users.simulation_deployer }} --output json
          register: deployer_keys

        - name: delete existing deployer access keys
          shell: >-
            aws iam delete-access-key --user-name {{ users.simulation_deployer }} \
              --access-key-id {{ item.AccessKeyId }}
          with_items:
            - "{{ (deployer_keys.stdout.strip() | from_json).AccessKeyMetadata }}"
        tags:
          - delete-existing-deployer-keys

      - name: download deployer access keys
        shell: >-
          aws iam create-access-key --user-name {{ users.simulation_deployer }} \
            --output json --query AccessKey
        register: sim_deployer_access_keys
        
      - name: transform credentials in recognazable format
        set_fact:
          deployer_creds:
            access_key_id: "{{ access_keys.AccessKeyId }}"
            secret_access_key: "{{ access_keys.SecretAccessKey }}"
        vars:
          access_keys: "{{ sim_deployer_access_keys.stdout | from_json }}"

      - name: save deployer access keys to file
        copy:
          dest: "{{ sim_deployer_config }}"
          mode: 0600
          content: "{{ deployer_creds | to_nice_json }}"
      when: should_create_deployer_keys

    - name: ensure variables directory exists
      file:
        path: "{{ default_vars_dir }}"
        state: directory

    - name: save created objects to disk, for further use
      copy:
        dest: "{{ default_setup_variables_file }}"
        mode: 0644
        backup: yes
        content: |
          ---
          my_cidr: {{ my_cidr }}
          image_id: {{ image_id }}
          key_id: {{ key_id }}
          sg_ids: {{ sg_ids }}
          subnet_id: {{ subnet_id }}
          subnet_cidr_block: {{ subnet_cidr_block }}
          volume_size: {{ volume_size }}